{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4215673,"sourceType":"datasetVersion","datasetId":2485074},{"sourceId":215554543,"sourceType":"kernelVersion"},{"sourceId":218819859,"sourceType":"kernelVersion"},{"sourceId":218820712,"sourceType":"kernelVersion"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom typing import Tuple, Optional\nfrom abc import ABC, abstractmethod\nimport io, datetime, hashlib\nimport gc\nimport warnings\n\nfrom luna16_dsets import ClassifierDataset, SegmenterDataset\nfrom luna16_model import Classifier, Segmenter\nfrom luna16_util import *","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Better way to calculate the ground truth masks\n# Experiment with different loss\n# Segmenter saves the last epoch, instead of best score [bug]\n# Memory is overflowed first time the segmenter is kicked off for training [bug]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Template Pattern Implementation\nclass BaseTrainer(ABC):\n    \"\"\"Base trainer the LUNA16 lung nodule detection model.\"\"\"\n    def __init__(self, config):\n        warnings.filterwarnings(\"ignore\")\n        self.config = config\n        self._setup()\n\n    def _init_cache(self):\n        pass\n        \n    @abstractmethod\n    def _init_criterion(self):\n        pass\n        \n    @abstractmethod\n    def _pre_epoch(self, epoch):\n        pass\n        \n    @abstractmethod\n    def _post_epoch(self, data, epoch):\n        pass\n        \n    @abstractmethod\n    def _process_batch(self):\n        pass\n        \n    def _setup(self):\n        self.device = torch.device('cuda' \n                                   if torch.cuda.is_available() \n                                   else 'cpu')\n        \n        self.batch_size = (torch.cuda.device_count() * self.config.batch_size \n                           if str(self.device) == 'cuda' \n                           else self.config.batch_size)\n        \n        self._init_cache()\n\n        self.train_loader = DataLoader(\n            self.Dataset(mode='train', config=self.config), \n            batch_size=self.batch_size, \n            num_workers=self.config.num_workers, \n            pin_memory=True, \n            shuffle=True\n        )\n                \n        self.val_loader = DataLoader(\n            self.Dataset(mode='val', config=self.config), \n            batch_size=self.batch_size, \n            num_workers=self.config.num_workers, \n            pin_memory=True\n        )\n\n        model = self.Model(batch_norm=self.config.batch_norm)\n        if torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)\n        self.model = model.to(self.device)\n        \n        self.optimizer = optim.Adam(self.model.parameters())\n        self.scaler = GradScaler()\n        self.criterion = self._init_criterion()\n\n    def train(self):\n        \"\"\"Main training loop.\"\"\"\n        print(\"Training started\")\n\n        timestamp = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')\n        self.writer = SummaryWriter(f\"runs/{self.config.model_type}_{timestamp}\")\n        \n        for epoch in range(1, self.config.epochs + 1):\n            # Training phase\n            self.model.train()\n            loss_acc = 0\n            samples_acc = 0\n            start_time = datetime.datetime.now().replace(microsecond=0)\n            self._pre_epoch(epoch)\n            \n            for i, batch in enumerate(self.train_loader):\n                loss = self._process_batch(batch, i)\n                loss_acc += loss.sum().item()\n                samples_acc += len(batch[0])\n                log_format = (\n                    f\"Epoch {epoch}, {datetime.datetime.now().replace(microsecond=0) - start_time} \"\n                    f\"{i+1}/{len(self.train_loader)}, Loss={loss_acc/samples_acc:.4f}\")\n                print(log_format, end='\\r')\n\n            print()\n            gc.collect()\n            \n            # Validation phase\n            self.model.eval()\n            with torch.no_grad():\n                for i, batch in enumerate(self.val_loader):\n                    _ = self._process_batch(batch, i, mode='val')\n                gc.collect()\n\n            self._post_epoch(batch, epoch)\n            \n        self.writer.close()\n\nclass ClassificationTrainer(BaseTrainer):\n    def __init__(self, config):\n        self.Dataset = ClassifierDataset\n        self.Model = Classifier\n        super().__init__(config)\n\n    def _init_cache(self):\n        if self.config.cache_in:\n            print(\"Caching the dataset\")\n            for _ in DataLoader(self.Dataset(mode='cache', config=self.config), \n                                batch_size=self.config.batch_size, \n                                num_workers=self.config.num_workers): \n                pass\n                \n    def _init_criterion(self):\n        return nn.CrossEntropyLoss(reduction='none')\n\n    def _pre_epoch(self, epoch):\n        if self.config.enable_balanced_decay:\n                self.train_loader.dataset.balanced_decay(epoch)\n\n        self.metrics_train = torch.zeros((self.config.n_metrics, len(self.train_loader.dataset)))\n        self.metrics_val = torch.zeros((self.config.n_metrics, len(self.val_loader.dataset)))\n        self.best_score = 0\n\n    def _post_epoch(self, data, epoch):\n        log_classifier_metrics(self.model, self.metrics_train, epoch, 'Train', self.writer)\n        metrics_dict = log_classifier_metrics(self.model, self.metrics_val, epoch, 'Val', self.writer)\n        featurelogger = FeatureMapLogger(self.model, self.writer, self.config.visualize)\n        featurelogger(data[0], epoch)\n        featurelogger.close()\n        if metrics_dict['metric/recall'] > self.best_score:\n            self.best_score = metrics_dict['metric/recall']\n            model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model\n            self.state = {\n                'hyperparameters': self.config.to_dict(),\n                'model': model.state_dict(),\n                'current_epoch': epoch,\n                'metrics_val': metrics_dict,\n                'timestamp': datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')\n            }\n        if self.config.save_state and self.config.epochs == epoch:\n            buffer = io.BytesIO()\n            torch.save(self.state, buffer)\n            sha1 = hashlib.sha1(buffer.getvalue()).hexdigest()\n            with open(f'classifier.{sha1}', 'wb') as f:\n                f.write(buffer.getvalue())\n    \n    def _process_batch(self, batch, i, mode='train'):\n        \"\"\"Process a single batch during training or validation.\"\"\"\n        x, y = batch[0].to(self.device), batch[1].to(self.device)\n        if hasattr(self.config, 'augment') and mode == 'train':\n            x, _ = augment_candidates_3d(x, y, self.config.augment)\n            \n        with autocast():\n            y_pred, prob = self.model(x)\n            loss = self.criterion(y_pred, y)\n        \n        if mode == 'train':\n            self.optimizer.zero_grad()\n            self.scaler.scale(loss.mean()).backward()\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n\n        metrics = torch.zeros((self.config.n_metrics, len(x)))\n        start, end = i * len(x), i * len(x) + len(x)\n        metrics[0] = loss.detach()\n        metrics[1] = prob[:, 1]\n        metrics[2] = y\n        if mode == 'train':\n            self.metrics_train[:,start:end] = metrics\n        elif mode == 'val':\n            self.metrics_val[:,start:end] = metrics\n            \n        return loss\n\nclass SegmentationTrainer(BaseTrainer):\n    def __init__(self, config):\n        self.Dataset = SegmenterDataset\n        self.Model = Segmenter\n        super().__init__(config)\n\n    def _init_cache(self):\n        pass\n        \n    def _init_criterion(self):\n        return F1MacroLoss()\n\n    def _pre_epoch(self, epoch):\n        self.metrics_train = torch.zeros((self.config.n_metrics, len(self.train_loader.dataset)))\n        self.metrics_val = torch.zeros((self.config.n_metrics, len(self.val_loader.dataset)))\n        self.best_score = 0\n\n    def _post_epoch(self, data, epoch):\n        log_segmenter_metrics(self.model, self.metrics_train, epoch, 'Train', self.writer)\n        metrics_dict = log_segmenter_metrics(self.model, self.metrics_val, epoch, 'Val', self.writer)\n        f1_macro = log_masked_image(data, self.model, self.writer, epoch, self.config.visualize)\n        if f1_macro > self.best_score:\n            self.best_score = f1_macro\n            model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model\n            self.state = {\n                'hyperparameters': self.config.to_dict(),\n                'model': model.state_dict(),\n                'current_epoch': epoch,\n                'metrics_val': metrics_dict,\n                'timestamp': datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')\n            }\n        if self.config.save_state and self.config.epochs == epoch:\n            buffer = io.BytesIO()\n            torch.save(self.state, buffer)\n            sha1 = hashlib.sha1(buffer.getvalue()).hexdigest()\n            print(f'Epoch {self.state[\"current_epoch\"]} is saved!')\n            with open(f'segmenter.{sha1}', 'wb') as f:\n                f.write(buffer.getvalue())\n\n    def _process_batch(self, batch, i, mode='train'):\n        \"\"\"Process a single batch during training or validation.\"\"\"\n        x, y = batch[0].to(self.device), batch[1].to(self.device)\n        if hasattr(self.config, 'augment') and mode == 'train':\n            x, y = augment_candidates_2d(x, y, self.config.augment)\n            \n        with autocast():\n            y_pred = self.model(x)\n            loss = self.criterion(y_pred, y)\n        \n        if mode == 'train':\n            self.optimizer.zero_grad()\n            self.scaler.scale(loss.mean()).backward()\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n\n        pos_mask = y == 1\n        neg_mask = ~pos_mask\n        true_pos = (y_pred.detach() * pos_mask).sum(dim=(2,3))\n        true_neg = ((1-y_pred.detach()) * neg_mask).sum(dim=(2,3))\n        false_pos = neg_mask.sum(dim=(2,3)) - true_neg\n        false_neg = pos_mask.sum(dim=(2,3)) - true_pos\n        metrics = torch.zeros((self.config.n_metrics,len(x)))\n        start, end = i * len(x), i * len(x) + len(x)\n        metrics[0] = loss.detach()\n        metrics[1] = true_pos.squeeze()\n        metrics[2] = true_neg.squeeze()\n        metrics[3] = false_pos.squeeze()\n        metrics[4] = false_neg.squeeze()\n        if mode == 'train':\n            self.metrics_train[:,start:end] = metrics\n        elif mode == 'val':\n            self.metrics_val[:,start:end] = metrics\n        \n        return loss\n\n#\n# Factory Pattern Implementation\nclass TrainingApp:\n    @staticmethod\n    def create_trainer(config):\n        trainers = {\n            'classification': ClassificationTrainer,\n            'segmentation': SegmentationTrainer\n        }\n        \n        trainer = trainers.get(config.model_type)\n        return trainer(config)\n\n#\ndef main():\n    \"\"\"Entry point for training.\"\"\"\n    hyper_parameters = {\n        'model_type': 'segmentation',  # 'segmentation' | 'classification'\n        'window': 'full_range',\n        'save_state': True,\n        'normalize': True,\n        'batch_norm': False,\n        'batch_size': 64,\n        'num_workers': 4,\n        'cache_in': True,\n        'visualize': True,\n        'epochs': 100,\n        'n_metrics': 5,\n        'balanced': 1,\n        'enable_balanced_decay': True,\n        'augment': {\n            'flip': True,\n            'offset': 0.1,\n            'scale': 0.2,\n            'rotate': True,\n            'noise': 0.1,\n            'mixup': 0.4\n        }\n    }\n\n    config = Config(hyper_parameters)\n    trainer = TrainingApp.create_trainer(config)\n    trainer.train()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r runs.zip runs/*","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}