{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4215673,"sourceType":"datasetVersion","datasetId":2485074},{"sourceId":10633574,"sourceType":"datasetVersion","datasetId":6583597},{"sourceId":215554543,"sourceType":"kernelVersion"},{"sourceId":220127630,"sourceType":"kernelVersion"},{"sourceId":220190293,"sourceType":"kernelVersion"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimport pandas as pd\nimport scipy.ndimage as ndimage\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport datetime\nimport warnings\nimport gc\nfrom glob import glob\n\nfrom luna16_dsets import ClassifierDataset, SegmenterDataset\nfrom luna16_model import Classifier, Segmenter\nfrom luna16_util import show_nodule, Config","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class InferenceApp:\n    \"\"\"LUNA16 lung nodule detection inference pipeline.\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self._setup_models()\n        \n    def _setup_models(self) -> None:\n        \"\"\"Initialize and load pre-trained models.\"\"\"\n        self.batch_size = (\n            torch.cuda.device_count() * self.config.batch_size \n            if torch.cuda.is_available() \n            else self.config.batch_size\n        )\n        \n        mhd_files = Path('/kaggle/input/luna16').glob(f'subset{self.config.subset}/subset*/*.mhd')\n        self.seriesuids = [p.stem for p in mhd_files]\n        \n        self.classifier = self._load_model(\n            Classifier(batch_norm=self.config.batch_norm),\n            '/kaggle/input/luna16-models/classifier.1946e06a83592641ed139455bd1a77154906bf0c'\n        )\n        \n        self.segmenter = self._load_model(\n            Segmenter(batch_norm=self.config.batch_norm),\n            '/kaggle/input/luna16-models/segmenter.ace47f0d1a9f7e595a0541132d50b1deac7f40f9'\n        )\n    \n    def _load_model(self, model: nn.Module, checkpoint_path: str) -> nn.Module:\n        \"\"\"Load model from checkpoint and move to device.\"\"\"\n        state = torch.load(checkpoint_path)\n        model.load_state_dict(state['model'])\n        if torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)\n        return model.to(self.device)\n    \n    def get_data_loader(self, seriesuid: Optional[str] = None, \n                       classify: bool = False, \n                       data: Optional[pd.DataFrame] = None) -> DataLoader:\n        \"\"\"Create appropriate DataLoader based on task.\"\"\"\n        if classify:\n            dataset = ClassifierDataset(\n                data=data, \n                mode='val', \n                config=self.config, \n                subset=self.config.subset\n            )\n        else:\n            dataset = SegmenterDataset(\n                seriesuid=seriesuid, \n                mode=self.config.mode, \n                config=self.config, \n                subset=self.config.subset\n            )\n            \n        return DataLoader(\n            dataset,\n            batch_size=self.batch_size,\n            num_workers=self.config.num_workers,\n            pin_memory=True\n        )\n    \n    def process_series(self, seriesuid: str) -> pd.DataFrame:\n        \"\"\"Process a single CT series.\"\"\"\n        # Segmentation\n        ds = self.get_data_loader(seriesuid)\n        x_full, y_pred = [], []\n        \n        for x, y, _, _ in ds:\n            x, y = x.to(self.device), y.to(self.device)\n            y_pred.append(self.segmenter(x).cpu().squeeze())\n            x_full.append(x[:,1].cpu())\n        \n        y_pred = torch.cat(y_pred, dim=0) > 0.5\n        x_full = torch.cat(x_full, dim=0)\n        y_pred = ndimage.binary_erosion(y_pred, iterations=1)\n        \n        # Find nodule candidates\n        label_array, count = ndimage.label(y_pred)\n        centers = ndimage.center_of_mass(\n            x_full,\n            labels=label_array,\n            index=np.arange(1, count + 1)\n        )\n        \n        # Classification\n        ct = SegmenterDataset._get_ct(\n            seriesuid, \n            self.config.window, \n            self.config.normalize, \n            self.config.subset\n        )\n        \n        candidates = pd.DataFrame(columns=['seriesuid', 'coordX', 'coordY', 'coordZ', 'class', 'diameter_mm'])\n        for irc in centers:\n            xyz = ct.irc2xyz(irc)\n            candidates.loc[len(candidates)] = [seriesuid, xyz[0], xyz[1], xyz[2], -1, -1]\n            \n        if not candidates.empty:\n            ds = self.get_data_loader(classify=True, data=candidates)\n            predictions = []\n            \n            for batch in ds:\n                x, y = batch[0].to(self.device), batch[1].to(self.device)\n                pred = self.classifier(x)[1][:,1].cpu()\n                predictions.append(pred)\n                \n                if self.config.visualize:\n                    show_nodule(batch[0][0].squeeze(1).cpu(), pred[0], batch[3][0].cpu())\n            \n            candidates['class_p'] = torch.cat(predictions, dim=0).numpy() > 0.5\n            \n        return candidates\n    \n    def infer(self) -> pd.DataFrame:\n        \"\"\"Run inference pipeline on all series.\"\"\"\n        warnings.filterwarnings(\"ignore\")\n        print(\"Starting inference...\")\n        \n        timestamp = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')\n        writer = SummaryWriter(f\"runs/{self.config.mode}_{timestamp}\")\n        \n        results = []\n        self.segmenter.eval()\n        self.classifier.eval()\n        \n        with torch.no_grad():\n            for seriesuid in self.seriesuids:\n                results.append(self.process_series(seriesuid))\n                gc.collect()\n                \n        writer.close()\n        return pd.concat(results, ignore_index=True)\n\ndef main():\n    \"\"\"Entry point for inference.\"\"\"\n    hyper_parameters = {\n        'mode': 'full_val',  # 'full_val' | 'inference'\n        'window': 'full_range',\n        'subset': 0,\n        'normalize': True,\n        'batch_norm': False,\n        'batch_size': 8,\n        'balanced': 0,\n        'num_workers': 4,\n        'cache_in': False,\n        'visualize': True,\n        'n_metrics': 5\n    }\n\n    config = Config(hyper_parameters)\n    \n    app = InferenceApp(config)\n    results = app.infer()\n    print(results)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}