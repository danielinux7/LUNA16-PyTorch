{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4215673,"sourceType":"datasetVersion","datasetId":2485074},{"sourceId":215554543,"sourceType":"kernelVersion"},{"sourceId":217118992,"sourceType":"kernelVersion"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install diskcache","metadata":{"_uuid":"ca52520b-87fc-4640-9ab1-47741e455b51","_cell_guid":"7ce37592-869e-454d-ac25-dd1028f8477d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nfrom skimage.segmentation import watershed\nimport torch\nimport SimpleITK as sitk\nfrom glob import glob\nfrom typing import Tuple, List, NamedTuple\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport diskcache\nimport functools\n\nfrom luna16_util import Config, augment_candidates_3d, augment_candidates_2d, show_nodule, create_3d_tomograph","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## UTILITY CLASSES","metadata":{}},{"cell_type":"code","source":"class XyzTuple(NamedTuple):\n    x: float\n    y: float\n    z: float\n\nclass IrcTuple(NamedTuple):\n    index: int\n    row: int\n    col: int\n\nclass Ct:\n    def __init__(self, series_uid, window, normalize, subset=0):\n        \"\"\"\n        Initialize CT scan object with series UID\n        \n        Args:\n            series_uid (str): Unique identifier for CT series\n        \"\"\"\n        mhd_path = glob(f'/kaggle/input/luna16/subset{subset}/subset*/{series_uid}.mhd')[0]\n        self.ct_mhd = sitk.ReadImage(mhd_path)\n        \n        # Process CT array\n        ct_a = np.array(sitk.GetArrayFromImage(self.ct_mhd), dtype=np.float32)\n        ct_a = self.apply_window(ct_a, window, normalize)\n        \n        self.series_uid = series_uid\n        self.hu_a = ct_a\n        self.origin_xyz = np.array(self.ct_mhd.GetOrigin())\n        self.vxSize_xyz = np.array(self.ct_mhd.GetSpacing())\n        self.direction_xyz = np.array(self.ct_mhd.GetDirection()).reshape(3, 3)\n\n    def irc2xyz(self, coord_irc: Tuple[int, int, int]) -> XyzTuple:\n        \"\"\"Convert image-row-column coordinates to physical xyz coordinates\"\"\"\n        cri_a = np.array(coord_irc)[::-1]\n        coords_xyz = (self.direction_xyz @ (cri_a * self.vxSize_xyz)) + self.origin_xyz\n        return XyzTuple(*coords_xyz)\n\n    def xyz2irc(self, coord_xyz: Tuple[float, float, float]) -> IrcTuple:\n        \"\"\"Convert physical xyz coordinates to image-row-column coordinates\"\"\"\n        cri_a = ((coord_xyz - self.origin_xyz) @ np.linalg.inv(self.direction_xyz)) / self.vxSize_xyz\n        cri_a = np.round(cri_a)\n        return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))\n\n    def apply_window(self, ct_scan, window_name, normalize=True):\n\n        window_settings = {\n            'full_range': {'center': 0, 'width': 2000},\n            'brain': {'center': 40, 'width': 80},\n            'soft_tissue': {'center': 50, 'width': 400},\n            'lung': {'center': -600, 'width': 1500},\n            'bone': {'center': 400, 'width': 1500},\n            'mediastinum': {'center': 50, 'width': 350}\n        }\n        \n        settings = window_settings[window_name]\n        center = settings['center']\n        width = settings['width']\n        \n        min_bound = center - width // 2\n        max_bound = center + width // 2\n        \n        ct_scan = np.clip(ct_scan, min_bound, max_bound)\n        if normalize:\n            ct_scan = (ct_scan - min_bound) / (width + 1e-8)\n            \n        return ct_scan","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ClassifierDataset","metadata":{}},{"cell_type":"code","source":"class ClassifierDataset(Dataset):\n    def __init__(self, seriesuid: str = None, width: Tuple[int, int, int] = (32, 48, 48), data=None,\n                 mode: str = 'train', cache_path: str = 'memoize_cache', config: Config = None, subset: int = 0):\n        \"\"\"\n        Initialize LUNA16 Dataset\n        \n        Args:\n            seriesuid (str, optional): Specific series UID to process\n            width (Tuple[int, int, int]): Volume extraction dimensions\n            show (bool): Whether to visualize nodules\n            mode (str): Dataset mode ('train', 'val', 'cache')\n        \"\"\"\n        self.data = data\n        self.width = width\n        self.mode = mode\n        self.subset = subset\n        self.seriesuid = seriesuid\n        self.window = config.window\n        self.normalize = config.normalize\n        self.meta_data, self.label_name = self._get_metadata()\n        if config.balanced and self.mode == 'train':\n            self.meta_data = self._balance_metadata(self.meta_data, self.label_name, config.balanced)\n        self.cache = diskcache.Cache(cache_path, size_limit=3e11)\n\n    def __len__(self) -> int:\n        return len(self.meta_data)\n\n    def __getitem__(self, idx: int):\n        row = self.meta_data.loc[idx]\n        seriesuid = row.seriesuid\n        xyz_center = row[['coordX', 'coordY', 'coordZ']].tolist()\n        @self.cache.memoize()\n        def _cached_getitem(seriesuid, xyz_center, width):\n            ct = self._get_ct(seriesuid, self.window, self.normalize, self.subset)\n            candidate, irc_center = self.get_candidate(ct ,xyz_center, width)\n            return candidate, row[self.label_name], row.seriesuid, (irc_center.index, irc_center.row, irc_center.col)\n\n        candidate, label, seriesuid, irc_center = _cached_getitem(seriesuid, xyz_center, self.width)\n        \n        return (\n            torch.from_numpy(candidate)[None],\n            torch.tensor(label, dtype=torch.long),\n            seriesuid, \n            torch.tensor(irc_center)\n        )\n\n    @functools.lru_cache(maxsize=1)\n    def _get_ct(self, seriesuid, window, normalize, subset):\n        return Ct(seriesuid, window, normalize)\n\n    def get_candidate(self, ct_scan: Ct, xyz_center: List[float], width: Tuple[int, int, int]) -> Tuple[np.ndarray, IrcTuple]:\n        \"\"\"\n        Extract candidate region around a center point\n        \n        Args:\n            xyz_center (List[float]): Center coordinates\n            width (Tuple[int, int, int]): Extraction dimensions\n        \n        Returns:\n            Tuple of extracted volume and center coordinates\n        \"\"\"\n        irc_center = ct_scan.xyz2irc(xyz_center)\n        candidate_wrap = np.zeros(width, dtype='float32')\n        candidate = ct_scan.hu_a[\n                irc_center.index - width[0]//2:irc_center.index + width[0]//2,\n                irc_center.row - width[1]//2:irc_center.row + width[1]//2,\n                irc_center.col - width[2]//2:irc_center.col + width[2]//2\n            ]\n        d,h,w = candidate.shape\n        candidate_wrap[:d,:h,:w] = candidate[:d,:h,:w]\n        return (\n            candidate_wrap,\n            irc_center\n        )\n        \n    def sampler(self, class_weight=[1,1]):\n        class_weights = class_probaility * 1 / self.meta_data[self.label_name].value_counts()\n        sample_weights = class_weights[self.meta_data[self.label_name]].reset_index(drop=True).astype('float32')\n        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(self.meta_data), replacement=True)\n        return sampler\n        \n    def _balance_metadata(self, df, target_column, ratio=1):\n\n        class_counts = df[target_column].value_counts()\n    \n        majority_class = class_counts.index[0]\n        minority_class = class_counts.index[-1]\n    \n        majority_df = df[df[target_column] == majority_class]\n        minority_df = df[df[target_column] == minority_class]\n    \n        samples_to_add = int(ratio * len(majority_df))\n        oversampled_minority = minority_df.sample(n=samples_to_add, replace=True)\n        balanced_df = pd.concat([majority_df, oversampled_minority], ignore_index=True)\n        \n        return balanced_df\n\n    def balanced_decay(self, epoch):\n        self.meta_data = self._balance_metadata(self.meta_data, self.label_name, 1/epoch**.3)\n        \n    def _get_metadata(self) -> pd.DataFrame:\n        \"\"\"\n        Process and merge candidate and annotation data\n        \n        Returns:\n            Processed DataFrame with nodule information\n        \"\"\"\n        if self.data is not None:\n            return self.data, 'class'\n            \n        candidates = pd.read_csv('/kaggle/input/luna16/candidates.csv')\n        annotations = pd.read_csv('/kaggle/input/luna16/annotations.csv')\n        \n        # Filter for available CT scans\n        mhd_list = glob(f'/kaggle/input/luna16/subset{self.subset}/subset*/*.mhd')\n        presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n        \n        candidates = candidates[candidates['seriesuid'].isin(presentOnDisk_set)]\n        annotations = annotations[annotations['seriesuid'].isin(presentOnDisk_set)]\n        \n        # Merge and process data\n        result = pd.merge(candidates, annotations, on=['seriesuid'], how='left')\n        result['diameter_mm'] = result['diameter_mm'].fillna(0)\n        \n        # Calculate distances\n        nodule_coords = result[['coordX_x','coordY_x','coordZ_x']].values\n        center = result[['coordX_y','coordY_y','coordZ_y']].values\n        distances = np.linalg.norm(nodule_coords - center, axis=1)\n        \n        result['distance'] = distances / (result['diameter_mm'] + 1e-15)\n        result['distance'] = result['distance'].fillna(100)\n        \n        result = (result\n                  .sort_values('distance')\n                  .groupby(['seriesuid', 'coordX_x', 'coordY_x', 'coordZ_x'])\n                  .first()\n                  .reset_index())\n        \n        result = result.rename(columns={\n            'coordX_x': 'coordX', \n            'coordY_x': 'coordY', \n            'coordZ_x': 'coordZ'\n        })\n        \n        result.loc[result['distance'] > 0.3, 'diameter_mm'] = 0\n        result = (result[list(candidates.columns) + ['diameter_mm']]\n                 .sort_values('diameter_mm', ascending=False)\n                 .reset_index(drop=True))\n        \n        if self.seriesuid:\n            result = result[result.seriesuid == self.seriesuid]\n        \n        # Split data into train/validation\n        if self.mode == 'val':\n            result = result[result.index % 10 == 0].reset_index(drop=True)\n        elif self.mode == 'train': \n            result = result[result.index % 10 != 0].reset_index(drop=True)\n        elif self.mode == 'cache': \n            result = result.sort_values('seriesuid').reset_index(drop=True)\n            \n        return result, 'class'    ","metadata":{"_uuid":"660aae0d-cc22-4606-887f-436256cc0cf4","_cell_guid":"08f96035-4624-4d7f-b517-503d15f61b85","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hyper_parameters = {\n#         'model_type': 'classification',  # or 'segmentation'\n#         'window': 'full_range',\n#         'normalize': True,\n#         'batch_norm': False,\n#         'batch_size': 64,\n#         'num_workers': 4,\n#         'cache_in': False,\n#         'visualize': True,\n#         'epochs': 10,\n#         'n_metrics': 3,\n#         'balanced': 1,\n#         'enable_balanced_decay': True,\n#         'augment': {\n#             'flip': True,\n#             'offset': 0.1,\n#             'scale': 0.2,\n#             'rotate': True,\n#             'noise': 0.1,\n#             'mixup': 0.4\n#     }\n# }\n# config = Config(hyper_parameters)\n# luna = ClassifierDataset(mode='val', config=config)\n# for x, y, _, irc_center in DataLoader(luna, batch_size=1, shuffle=True):\n#     x, y = augment_candidates_3d(x, y, config.augment)\n#     show_nodule(x.squeeze(1), y, irc_center)\n#     create_3d_tomograph(x, slice_spacing=1, colormap='plasma', transparency=1.0, threshold=0)\n#     break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SegmenterDataset","metadata":{}},{"cell_type":"code","source":"cache = diskcache.Cache('memoize_cache', size_limit=3e11)\n\nclass SegmenterDataset(Dataset):\n    def __init__(self, seriesuid: str = None, img_size: Tuple[int, int] = (96, 96), crop_size: Tuple[int, int] = (64, 64), \n                 mode: str = 'train', config: Config = None, subset=0, threshold=.2):\n        \"\"\"\n        Initialize LUNA16 Dataset\n        \n        Args:\n            seriesuid (str, optional): Specific series UID to process\n            width (Tuple[int, int, int]): Volume extraction dimensions\n            show (bool): Whether to visualize nodules\n            mode (str): Dataset mode ('train', 'val')\n        \"\"\"\n        self.threshold = threshold\n        self.subset = subset\n        self.crop_size = crop_size\n        self.img_size = img_size\n        self.mode = mode\n        self.seriesuid = seriesuid\n        self.window = config.window\n        self.normalize = config.normalize\n        self.meta_data = self._get_metadata()\n        self.meta_data = self.populate_annotations(self.meta_data)\n\n    def __len__(self) -> int:\n        return len(self.meta_data)\n\n    def __getitem__(self, idx: int):\n        row = self.meta_data.loc[idx]\n        seriesuid = row.seriesuid\n        x, y, z = row[['coordX', 'coordY', 'coordZ']].tolist()\n        slice_idx = row.slice_idx\n        if self.mode in ['full_val','inference']:\n            nodule, full_mask = self.get_full_nodule_mask(seriesuid, self.meta_data, self.window, self.normalize)\n            irc_center = [0,0,0]\n        else:\n            nodule, full_mask, irc_center = self.get_nodule_mask(seriesuid, x, y, z, self.mode, self.img_size, self.window, self.normalize, self.subset)\n        if self.mode == 'train':\n            h_img, w_img = self.img_size\n            h_crop, w_crop = self.crop_size\n            h_start = h_img//2 - h_crop//2\n            w_start = w_img//2 - w_crop//2\n            h_start = random.randint(0, h_start)\n            w_start = random.randint(0, w_start)\n            nodule_slice = nodule[slice_idx-1:slice_idx+2, h_start:h_start+h_crop, w_start:w_start+w_crop]\n            mask = full_mask[slice_idx, h_start:h_start+h_crop, w_start:w_start+w_crop]\n        elif self.mode in ['val','full_val','inference']:\n            nodule_slice = nodule[slice_idx-1:slice_idx+2]\n            mask = np.zeros((nodule_slice.shape[1],nodule_slice.shape[2]), dtype='float32') if full_mask is None else full_mask[slice_idx]\n        \n        return (\n            torch.from_numpy(nodule_slice),\n            torch.tensor(mask, dtype=torch.float)[None],\n            seriesuid, \n            torch.tensor(irc_center)\n        )\n\n    def _calculate_mask(self, x, y, z, ct):\n        irc_center = ct.xyz2irc((x, y, z))\n        center_value = ct.hu_a[irc_center.index, irc_center.row, irc_center.col]\n        for i in range(16):\n            next_value_u = ct.hu_a[irc_center.index+i, irc_center.row, irc_center.col]\n            next_value_d = ct.hu_a[irc_center.index-i, irc_center.row, irc_center.col]\n            if next_value_u < self.threshold or next_value_d < self.threshold:\n                break\n\n        for j in range(16):\n            next_value_u = ct.hu_a[irc_center.index, irc_center.row+j, irc_center.col]\n            next_value_d = ct.hu_a[irc_center.index, irc_center.row-j, irc_center.col]\n            if next_value_u < self.threshold or next_value_d < self.threshold:\n                break\n                \n        for k in range(16):\n            next_value_u = ct.hu_a[irc_center.index, irc_center.row, irc_center.col+k]\n            next_value_d = ct.hu_a[irc_center.index, irc_center.row, irc_center.col-k]\n            if next_value_u < self.threshold or next_value_d < self.threshold:\n                break\n                \n        nodule = ct.hu_a[irc_center.index - i - 1:irc_center.index + i + 2]\n        mask = np.zeros_like(nodule, dtype=int)\n        mask[:,irc_center.row - j - 1:irc_center.row + j + 1,irc_center.col - k - 1:irc_center.col + k + 1] = 1\n        mask = mask * (nodule >= self.threshold)\n        return mask, irc_center.index - i - 1, irc_center.index + i + 2\n\n    @cache.memoize(ignore=(0,2,3,4))\n    def get_full_nodule_mask(self, seriesuid, meta_data, window, normalize):\n        ct = SegmenterDataset._get_ct(seriesuid, window, normalize, self.subset)\n        if self.mode == 'inference':\n            return ct.hu_a, None\n        XYZ = meta_data[meta_data.seriesuid == seriesuid][['coordX','coordY','coordZ']].values\n        full_mask = np.zeros_like(ct.hu_a, dtype=int)\n        for xyz in XYZ:\n            x, y, z = xyz\n            mask, start, end = self._calculate_mask(x, y, z, ct)\n            full_mask[start:end] = mask\n        \n        return ct.hu_a, full_mask\n\n    @cache.memoize(ignore=(0,5,6,7,8,9))\n    def get_nodule_mask(self, seriesuid, x, y, z, mode, img_size, window, normalize, subset):\n        ct = SegmenterDataset._get_ct(seriesuid, window, normalize, subset)\n        irc_center = ct.xyz2irc((x, y, z))\n        h, w = img_size\n        mask, start, end = self._calculate_mask(x, y, z, ct)\n        nodule = ct.hu_a[start:end]\n\n        if mode == 'train':\n            nodule = nodule[:,irc_center.row - h//2:irc_center.row + h//2, irc_center.col - w//2:irc_center.col + w//2]\n            mask = mask[:,irc_center.row - h//2:irc_center.row + h//2, irc_center.col - w//2:irc_center.col + w//2]\n\n        return nodule, mask, irc_center\n\n    @staticmethod\n    @functools.lru_cache(maxsize=1)\n    def _get_ct(seriesuid, window, normalize, subset):\n        return Ct(seriesuid, window, normalize, subset)\n\n    def populate_annotations(self, metadata):\n        slice_idx = []\n        if self.mode == 'inference':\n            for index, row in metadata.iterrows():\n                ct_scan, _ = self.get_full_nodule_mask(row.seriesuid, metadata, self.window, self.normalize)\n                slice_idx.append(list(range(1,ct_scan.shape[0] - 1)))\n\n            metadata['slice_idx'] = slice_idx\n            metadata = metadata.explode('slice_idx').reset_index(drop=True)\n            return metadata\n        if self.mode == 'full_val':\n            df = pd.DataFrame(metadata.groupby('seriesuid').first()).reset_index()\n            for index, row in df.iterrows():\n                _, full_mask = self.get_full_nodule_mask(row.seriesuid, metadata, self.window, self.normalize)\n                slice_idx.append(list(range(1,full_mask.shape[0] - 1)))\n\n            df['slice_idx'] = slice_idx\n            df = df.explode('slice_idx').reset_index(drop=True)\n            return df\n        else:\n            for index, row in metadata.iterrows():\n                x, y, z = row[['coordX', 'coordY', 'coordZ']].tolist()\n                _, full_mask, _  = self.get_nodule_mask(row.seriesuid, x, y, z, self.mode, self.img_size, self.window, self.normalize, self.subset)\n                slice_idx.append(list(range(1,full_mask.shape[0] - 1)))\n        \n            metadata['slice_idx'] = slice_idx\n            metadata = metadata.explode('slice_idx').reset_index(drop=True)\n            return metadata\n    \n    def _get_metadata(self) -> pd.DataFrame:\n        \"\"\"\n        Process and merge candidate and annotation data\n        \n        Returns:\n            Processed DataFrame with nodule information\n        \"\"\"\n        # Filter for available CT scans\n        mhd_list = glob(f'/kaggle/input/luna16/subset{self.subset}/subset*/*.mhd')\n        presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n        if self.mode == 'inference':\n            result = pd.DataFrame({'seriesuid': list(presentOnDisk_set)})\n            result[['coordX','coordY','coordZ','class','diameter_mm']] = None\n            if self.seriesuid:\n                result = result[result.seriesuid == self.seriesuid]\n            return result\n        candidates = pd.read_csv('/kaggle/input/luna16/candidates.csv')\n        annotations = pd.read_csv('/kaggle/input/luna16/annotations.csv')\n        \n        candidates = candidates[candidates['seriesuid'].isin(presentOnDisk_set)]\n        annotations = annotations[annotations['seriesuid'].isin(presentOnDisk_set)]\n        \n        # Merge and process data\n        result = pd.merge(candidates, annotations, on=['seriesuid'], how='left')\n        result['diameter_mm'] = result['diameter_mm'].fillna(0)\n        \n        # Calculate distances\n        nodule_coords = result[['coordX_x','coordY_x','coordZ_x']].values\n        center = result[['coordX_y','coordY_y','coordZ_y']].values\n        distances = np.linalg.norm(nodule_coords - center, axis=1)\n        \n        result['distance'] = distances / (result['diameter_mm'] + 1e-15)\n        result['distance'] = result['distance'].fillna(100)\n        \n        result = (result\n                  .sort_values('distance')\n                  .groupby(['seriesuid', 'coordX_x', 'coordY_x', 'coordZ_x'])\n                  .first()\n                  .reset_index())\n        \n        result = result.rename(columns={\n            'coordX_x': 'coordX', \n            'coordY_x': 'coordY', \n            'coordZ_x': 'coordZ'\n        })\n        \n        result.loc[result['distance'] > 0.3, 'diameter_mm'] = 0\n        result = (result[list(candidates.columns) + ['diameter_mm']]\n                 .sort_values('diameter_mm', ascending=False)\n                 .reset_index(drop=True))\n\n        result = result[result['class'] == 1]\n        seriesuid_pos = result.seriesuid.unique()\n        \n        # Split data into train/validation\n        if self.mode in ['val']:\n            indeces = np.array(range(len(seriesuid_pos))) % 10 == 0\n            seriesuid_pos = seriesuid_pos[indeces]\n            result = result.loc[result.seriesuid.isin(seriesuid_pos)]\n            result = result.sort_values('seriesuid')\n\n        elif self.mode in ['full_val']:\n            result = result.sort_values('seriesuid')\n            \n        elif self.mode == 'train': \n            indeces = np.array(range(len(seriesuid_pos))) % 10 != 0\n            seriesuid_pos = seriesuid_pos[indeces]\n            result = result.loc[result.seriesuid.isin(seriesuid_pos)]\n            result = result.sort_values('seriesuid')\n\n        if self.seriesuid:\n            result = result[result.seriesuid == self.seriesuid]\n        \n        return result.reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hyper_parameters = {\n#         'model_type': 'segmentation',  # or 'segmentation'\n#         'window': 'full_range',\n#         'normalize': True,\n#         'batch_norm': False,\n#         'batch_size': 64,\n#         'num_workers': 4,\n#         'cache_in': False,\n#         'visualize': True,\n#         'epochs': 10,\n#         'n_metrics': 3,\n#         'balanced': 1,\n#         'enable_balanced_decay': True,\n#         'augment': {\n#             'flip': True,\n#             'offset': 0.1,\n#             'scale': 0.2,\n#             'rotate': True,\n#             'noise': 0.1,\n#             'mixup': 0.4\n#     }\n# }\n# config = Config(hyper_parameters)\n\n# # validation seriesuid\n# seriesuid_val =  '1.3.6.1.4.1.14519.5.2.1.6279.6001.213140617640021803112060161074'\n# # luna = SegmenterDataset(seriesuid=seriesuid_val, mode='val', config=config)\n\n# # inference seriesuid\n# # seriesuid_inf =  '1.3.6.1.4.1.14519.5.2.1.6279.6001.100684836163890911914061745866'\n# # luna = SegmenterDataset(seriesuid=seriesuid_inf, mode='inference', config=config, subset=1)\n\n# # train seriesuid\n# # seriesuid_train = '1.3.6.1.4.1.14519.5.2.1.6279.6001.128023902651233986592378348912'\n# # seriesuid_train = '1.3.6.1.4.1.14519.5.2.1.6279.6001.134996872583497382954024478441'\n# seriesuid_train = '1.3.6.1.4.1.14519.5.2.1.6279.6001.137763212752154081977261297097'\n# # seriesuid_train = '1.3.6.1.4.1.14519.5.2.1.6279.6001.137763212752154081977261297097'\n# luna = SegmenterDataset(seriesuid=seriesuid_train, mode='train', config=config)\n# # luna = SegmenterDataset(mode='train', config=config)\n\n# count = 0\n# for x, y, _, irc_center in DataLoader(luna, batch_size=1, shuffle=False):\n#     if count%10 == 0:\n#         # x, y = augment_candidates_2d(x,y,config.augment)\n#         show_nodule(x, 1, irc_center)\n#         show_nodule(y, 1, irc_center)\n#         # create_3d_tomograph(x, slice_spacing=1, colormap='plasma', transparency=1.0, threshold=0)\n#     count += 1","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}